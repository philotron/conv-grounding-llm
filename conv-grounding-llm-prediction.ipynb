{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa167f6",
   "metadata": {},
   "source": [
    "# Experiments: Conversational Grounding LLM Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49bcf83",
   "metadata": {},
   "source": [
    "This jupyter notebook contains the code for the experiments for the paper titled: \"Towards Harnessing Large Language Models for Enhanced Comprehension of Conversational Grounding\" submitted to IWSDS 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbc50f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d470725a",
   "metadata": {},
   "source": [
    "### Load Dialogue Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'annotated_dialogues.xlsx'\n",
    "df = pd.read_excel(file_path, header=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframe to specific dialogue examples, e.g. d1 with 'channel-02' and topic 'media'\n",
    "df_d1 = df[(df['room_name'] == 'channel-02') & (df['topic'].str.lower() == 'media')]\n",
    "df_d1 = df_d1.sort_values(by='timestamp', ascending=True)\n",
    "\n",
    "df_d2 = df[(df['room_name'] == 'channel-02') & (df['topic'].str.lower() == 'geography')]\n",
    "df_d2 = df_d2.sort_values(by='timestamp', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dialogue into list of input subdialogues for LLM\n",
    "# Create list with indices for the LLM predictions\n",
    "def generate_input_dialogue_list(df, label_indices):\n",
    "    input_dialogue_list = []\n",
    "    current_dialogue = []\n",
    "    for i in label_indices:\n",
    "        current_dialogue = []\n",
    "        for index, row in df[[\"role\", \"message\", \"grounding_label\", \"grounded_knowledge\"]].iterrows():\n",
    "            role = row['role']\n",
    "            message = row['message']\n",
    "            grounding_label = row['grounding_label']\n",
    "            grounded_knowledge = row['grounded_knowledge']\n",
    "\n",
    "            current_dialogue.append(f\"{role}: {message}\")\n",
    "\n",
    "            if index == i:\n",
    "                input_dialogue_list.append({'label_index': i, 'dialogue': current_dialogue, 'grounding_label': grounding_label, 'grounded_knowledge': grounded_knowledge})\n",
    "                break\n",
    "    return input_dialogue_list\n",
    "\n",
    "label_indices_d1 = df_d1[df_d1['grounding_label'].isin([\"explicit\", \"implicit\", \"clarification\"])].index.tolist()\n",
    "input_dialogue_d1 = generate_input_dialogue_list(df_d1, label_indices_d1)\n",
    "\n",
    "label_indices_d2 = df_d2[df_d2['grounding_label'].isin([\"explicit\", \"implicit\", \"clarification\"])].index.tolist()\n",
    "input_dialogue_d2 = generate_input_dialogue_list(df_d2, label_indices_d2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d450cc",
   "metadata": {},
   "source": [
    "### Define Prompts for LLM in Chat Format\n",
    "\n",
    "* CLS = classification of grounding label\n",
    "* IE = information extraction of grounded knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CLS prompts\n",
    "def get_CLS_system_prompt() -> str:\n",
    "    ''' Returns the system instruction for CLS of grounding labels'''\n",
    "    return \"Predict the grounding label, representing when knowledge has been mutually grounded, for the last turn in the 'Input dialogue:'. The label can be 'explicit' if knowledge is verbally accepted, 'implicit' if accepted by moving forward with the conversation, or 'clarification' if a previous utterance must be clarified before acceptance.\"\n",
    "\n",
    "def get_CLS_zero_shot_chat_prompt(input_dialogue: str, system_message: bool = True):\n",
    "    ''' Returns the template for the zero-shot prompt that predicts a grounding label given a dialogue'''\n",
    "    message = []\n",
    "\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_CLS_system_prompt()}\"})\n",
    "\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output label: \"\"\"})\n",
    "    return message\n",
    "\n",
    "user_CLS_example_1 = \"\"\"\n",
    "Input dialogue:\n",
    "seeker: Can you tell me about the dataset's content?\n",
    "provider: The dataset contains information about planets in our solar system.\n",
    "seeker: What is the number of columns in the dataset?\n",
    "\"\"\"\n",
    "assistant_CLS_example_1 = \"\"\"\n",
    "Output label: implicit\n",
    "\"\"\"\n",
    "user_CLS_example_2 = \"\"\"\n",
    "Input dialogue:\n",
    "provider: My dataset has 191 rows and several columns.\n",
    "provider: There is a column for the human development index.\n",
    "seeker: But what does it represent and how is this index calculated?\n",
    "\"\"\"\n",
    "assistant_CLS_example_2 = \"\"\"\n",
    "Output label: clarification\n",
    "\"\"\"\n",
    "user_CLS_example_3 = \"\"\"\n",
    "Input dialogue:\n",
    "provider: The Varso Tower is the tallest building in the EU.\n",
    "seeker: Okay, thanks.\n",
    "\"\"\"\n",
    "assistant_CLS_example_3 = \"\"\"\n",
    "Output label: explicit\n",
    "\"\"\"\n",
    "\n",
    "def get_CLS_few_shot_chat_prompt(input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the few-shot prompt that predicts a grounding label given a dialogue'''\n",
    "    message = []\n",
    "    # system instruction\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_CLS_system_prompt()}\"})\n",
    "    # few-shot examples\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_CLS_example_3}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_CLS_example_3}\"\"\"})\n",
    "    # input dialogue\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output label: \"\"\"})\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6891d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IE prompts\n",
    "def get_IE_system_prompt() -> str:\n",
    "    ''' Returns the system instruction for IE of grounded knowledge'''\n",
    "    return \"Predict the newly grounded knowledge for the last turn in the 'Input dialogue:'. Use the JSON structure: {'table_domain': str, 'table_content': str, 'row_count': int, 'column_count': int, 'column_info': [{'column_name': str, 'values': [], 'distinct_count': int, 'min_value': int, 'max_value': int}]}. Adhere strictly to the JSON structure, and only predict the attributes mentioned in the dialogue turns, leaving unmentioned attributes as null.\"\n",
    "\n",
    "def get_IE_zero_shot_chat_prompt(input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the zero-shot prompt that predicts a JSON with grounded knowledge given a dialogue'''\n",
    "    message = []\n",
    "\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_IE_system_prompt()}\"})\n",
    "    \n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output JSON: \"\"\"})\n",
    "    return message\n",
    "\n",
    "\n",
    "user_IE_example_1 = \"\"\"\n",
    "Input dialogue:\n",
    "seeker: Can you tell me about the dataset's content?\n",
    "provider: The dataset contains information about planets in our solar system.\n",
    "seeker: What is the number of columns in the dataset?\n",
    "\"\"\"\n",
    "assistant_IE_example_1 = \"\"\"\n",
    "Output JSON: {'table_content': 'planets of the solar system'}\n",
    "\"\"\"\n",
    "user_IE_example_2 = \"\"\"\n",
    "Input dialogue:\n",
    "provider: My dataset has 191 rows and several columns.\n",
    "provider: There is a column for the human development index.\n",
    "seeker: But how is this index calculated and what does it mean?\n",
    "\"\"\"\n",
    "assistant_IE_example_2 = \"\"\"\n",
    "Output JSON: {'row_count': 191, 'column_info': [{'column_name': 'human development index', 'description': null}]}\n",
    "\"\"\"\n",
    "user_IE_example_3 = \"\"\"\n",
    "Input dialogue:\n",
    "provider: One column contains data about the height of the building in meters.\n",
    "provider: The Varso Tower is the tallest building in the dataset with 310 m.\n",
    "seeker: Okay, thanks.\n",
    "\"\"\"\n",
    "assistant_IE_example_3 = \"\"\"\n",
    "Output JSON: {'column_info': [{'column_name': 'height', 'description': 'height in meters', 'max_value': 310}]}\n",
    "\"\"\"\n",
    "\n",
    "def get_IE_few_shot_chat_prompt(input_dialogue: str, system_message: bool = True) -> list:\n",
    "    ''' Returns the template for the few-shot prompt that predicts a JSON with grounded knowledge given a dialogue'''\n",
    "    message = []\n",
    "    # system instruction\n",
    "    if system_message:\n",
    "        message.append({\"role\": \"system\", \"content\": f\"{get_IE_system_prompt()}\"})\n",
    "    # few-shot examples\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_IE_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_IE_example_1}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_IE_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_IE_example_2}\"\"\"})\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"{user_IE_example_3}\"\"\"})\n",
    "    message.append({\"role\": \"assistant\", \"content\": f\"\"\"{assistant_IE_example_3}\"\"\"})\n",
    "    # input dialogue\n",
    "    message.append({\"role\": \"user\", \"content\": f\"\"\"Input dialogue: {input_dialogue}\n",
    "Output JSON: \"\"\"})\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f48382",
   "metadata": {},
   "source": [
    "### Send Prompt to LLM (OpenAI API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b49204",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "def query_gpt(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        max_tokens = 256,\n",
    "        model=\"gpt-3.5-turbo-1106\",  # set model version = gpt-3.5-turbo-1106\n",
    "        messages=prompt, # provide prompt in chat format\n",
    "        temperature=0) # # set model temperature = 0\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68635941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference to get LLM predictions\n",
    "def run_CLS_inference(df, label_indices, input_dialogue, n_last_turns=None):\n",
    "    df_predictions = df\n",
    "    df_predictions[\"CLS_prediction\"] = \"\"\n",
    "    df_predictions[\"CLS_prompt\"] = \"\"\n",
    "    # n_last_turns = 100 # number of last input turns before last dialogue utterance\n",
    "    label_indices = label_indices\n",
    "    for idx in label_indices:\n",
    "        print(\"label index:\", idx)\n",
    "        if n_last_turns: # provide number of last input turns before last dialogue utterance\n",
    "            context_dialogue = \"\\n\".join(input_dialogue[label_indices.index(idx)][\"dialogue\"][-(n_last_turns+1):])\n",
    "        else: # provide full dialogue history\n",
    "            context_dialogue = \"\\n\".join(input_dialogue[label_indices.index(idx)][\"dialogue\"])\n",
    "        prompt = get_CLS_few_shot_chat_prompt(context_dialogue)\n",
    "        print(\"prompt:\", prompt)\n",
    "        response = query_gpt(prompt=prompt)\n",
    "        result = response.choices[0].message['content'].strip()\n",
    "        print(\"prediction:\", result)\n",
    "        print(\"\\n\\n\")\n",
    "        df_predictions.at[idx, \"CLS_prompt\"] = prompt\n",
    "        df_predictions.at[idx, \"CLS_prediction\"] = result\n",
    "        time_string = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime(int(time.time())))\n",
    "        df_predictions.to_excel(\"CLS_predictions_\" + time_string + \".xlsx\", index=False)\n",
    "        time.sleep(8)\n",
    "    return df_predictions\n",
    "\n",
    "def run_IE_inference(df, label_indices, input_dialogue, n_last_turns=None):\n",
    "    df_predictions = df\n",
    "    df_predictions[\"IE_prediction\"] = \"\"\n",
    "    df_predictions[\"IE_prompt\"] = \"\"\n",
    "    # n_last_turns = # number of last input turns before last dialogue utterance\n",
    "    label_indices = label_indices\n",
    "    for idx in label_indices:\n",
    "        print(\"label index:\", idx)\n",
    "        if n_last_turns: # provide number of last input turns before last dialogue utterance\n",
    "            context_dialogue = \"\\n\".join(input_dialogue[label_indices.index(idx)][\"dialogue\"][-(n_last_turns+1):])\n",
    "        else: # if None: provide full dialogue history\n",
    "            context_dialogue = \"\\n\".join(input_dialogue[label_indices.index(idx)][\"dialogue\"])\n",
    "        prompt = get_IE_few_shot_chat_prompt(context_dialogue)\n",
    "        print(\"prompt:\", prompt)\n",
    "        response = query_gpt(prompt=prompt)\n",
    "        result = response.choices[0].message['content'].strip()\n",
    "        print(\"prediction:\", result)\n",
    "        print(\"\\n\")\n",
    "        df_predictions.at[idx, \"IE_prompt\"] = prompt\n",
    "        df_predictions.at[idx, \"IE_prediction\"] = result\n",
    "        time_string = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime(int(time.time())))\n",
    "        df_predictions.to_excel(\"IE_predictions_\" + time_string + \".xlsx\", index=False)\n",
    "        time.sleep(8)\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c473d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CLS_predictions_d1 = run_CLS_inference(df=df_d1, label_indices=label_indices_d1, input_dialogue=input_dialogue_d1, n_last_turns=None)\n",
    "# df_CLS_predictions_d2 = run_CLS_inference(df=df_d2, label_indices=label_indices_d2, input_dialogue=input_dialogue_d2, n_last_turns=None)\n",
    "# df_IE_predictions_d1 = run_IE_inference(df=df_d1, label_indices=label_indices_d1, input_dialogue=input_dialogue_d1, n_last_turns=None)\n",
    "# df_IE_predictions_d2 = run_IE_inference(df=df_d2, label_indices=label_indices_d2, input_dialogue=input_dialogue_d2, n_last_turns=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
